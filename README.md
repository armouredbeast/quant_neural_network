# Neural Network From Scratch (NumPy)

This project implements a **feedforward neural network from first principles** using only NumPy.

No PyTorch.  
No TensorFlow.  
No autograd.

The objective is **mathematical transparency and engineering clarity**, not benchmark performance.

---

## ðŸŽ¯ What this project demonstrates

- Forward propagation using matrix operations
- Backpropagation derived directly from the chain rule
- Gradient descent optimization
- Modular neural network design
- Clean experiment separation
- Proper Python package structure

This repository is intended to show **understanding**, not abstraction.

---

## ðŸ“‚ Project Structure
quant_nn/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers.py          # Linear (dense) layer
â”‚   â”œâ”€â”€ activations.py     # ReLU activation
â”‚   â”œâ”€â”€ loss.py            # Mean Squared Error
â”‚   â””â”€â”€ network.py         # Forward/backward orchestration
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ synthetic_data.py  # Reproducible toy datasets
â”‚
â”œâ”€â”€ experiments/
â”‚   â””â”€â”€ run_experiment.py  # Training experiments
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ math.md

---

## ðŸ§  Model Architecture

For the default experiment:
Input (2)
â†“
Linear (2 â†’ 32)
â†“
ReLU
â†“
Linear (32 â†’ 1)

Loss function:
- Mean Squared Error (MSE)

Optimizer:
- Vanilla Gradient Descent (SGD-style)

---

## â–¶ï¸ How to Run

### 1. Create virtual environment (optional but recommended)

```bash
python3 -m venv venv
source venv/bin/activate

pip install -r requirements.txt

python -m experiments.run_experiment


### ðŸ“ˆ Expected Output

You should see the loss decrease over epochs:
epoch=0     loss â‰ˆ 10+
epoch=300   loss â†“
epoch=1000  loss significantly lower


---

# ðŸ“„ `requirements.txt`

Keep this **minimal**. Thatâ€™s a signal.

```text
numpy>=1.26